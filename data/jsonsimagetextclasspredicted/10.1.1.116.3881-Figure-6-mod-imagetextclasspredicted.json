{"Caption": "Figure 6: Evidence vs. number of training patches.", "CaptionBB": [87, 366, 388, 380], "Mention": ["We first performed a set of experiments to compare the \nepitomic training using the informative patch selection \nwith the training using random patch selection. For these \nexperiments, we used the spectrogram shown in the figure \n3. Figure 6 compares the likelihood of the input \nspectrogram given the epitomes trained using both the \nmethods while varying the number of patches used for \ntraining. The higher likelihood corresponds to a better \nexplanation of the input signal using the epitome. We \naveraged over 10 runs for each point in the curve. We can \nsee that the epitome using the informative sampling always \nexplains the input better than the epitome trained using \nrandom sampling. The difference is more prominent when \nthe number of patches is small. Naturally, as the number of \npatches goes to infinity the curves will meet. \n   Next, we demonstrate speech detection on an outdoor \nsequence consisting of speech with significant background \nnoise from nearby cars. We generated a 1 minute long \nepitome using 8 minutes of data. The speech class was \ntrained as described in 3.1.2 using only 5 labeled examples \nof speech. Figure 7 shows the result of applying speech \ndetection to a 10 second long audio sequence. The \ndetector does a good job of isolating speech segments \nfrom the non-speech segments in very significant noise \n(around -10dB SSNR; this and other data can be heard at \nhttp://research.microsoft.com/~sumitb/ae ). Note that there \n"], "ImageBB": [86, 136, 381, 352], "ImageText": [], "Number": 6, "GoldLabelsTagged": true, "Height": 1100, "Width": 850, "SomethingWrong": true, "Type": "Figure", "Page": 4, "DPI": 100}